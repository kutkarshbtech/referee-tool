---
# Referee Tool: LLM Model Recommendation System

## Overview
This project implements **Week 6: The Referee** challenge.  
It is designed to support research and decisionâ€‘making when selecting large language models (LLMs).  
The tool helps users choose the most suitable LLM for their project by:
- Extracting requirements from a natural language description (Claude via AWS Bedrock).
- Scoring and ranking models based on metadata in `data/Model_info.xlsx`.
- Explaining tradeâ€‘offs for each recommendation.
- Storing results in a Neo4j knowledge graph for future analysis and reuse.

Instead of providing a single answer, the tool acts as a **referee** â€” comparing options, explaining tradeâ€‘offs, and enabling informed decisions with more resources to start similar problem statements.

---

## Features
- **Requirement Extraction**: Uses Claude (AWS Bedrock) to analyze project descriptions and identify relevant fields.  
- **Model Scoring**: Normalizes metrics (latency, cost, reasoning, throughput) and ranks models.  
- **Tradeâ€‘off Explanations**: Provides concise summaries of why each model fits or doesnâ€™t fit.  
- **Knowledge Graph Storage**: Saves decisions in Neo4j AuraDB using Cypher queries.  
- **Interactive UI**: Streamlit app for submitting descriptions, viewing recommendations, and pushing results to Neo4j.

---

## Project Structure

referee-tool/
â”‚
â”œâ”€â”€ .kiro/                # Required directory for submission
â”‚   â””â”€â”€ config.json
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Model_info.xlsx   # LLM model metadata
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cypher_query.py       # Cypher query builder
â”‚   â”œâ”€â”€ keyword_extraction.py # Claude prompt + AWS Bedrock invocation
â”‚   â”œâ”€â”€ recommend.py          # Scoring and ranking logic
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ claude_output.json    # Saved Claude response
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_recommend.py     # Unit tests
â”‚
â”œâ”€â”€ main.py                   # Streamlit app entry point
â”œâ”€â”€ requirements.txt          # Dependencies
â””â”€â”€ README.md                 # Documentation


---

## How to Run

1. **Clone the repo**
git clone https://github.com/kutkarshbtech/referee-tool.git
cd referee-tool

2. **Install dependencies**
pip install -r requirements.txt

3. **Run the Streamlit app**
streamlit run main.py

---

## Example Usage

**Input:**
We need a low-latency, cost-efficient LLM for real-time chatbot deployment.

**Output:**
- ğŸ† Recommended Model: **Model_X**
- **Why:**
  - **Latency**: Fast response time ideal for real-time use  
  - **Cost**: Low input price per 100K tokens  
  - **Reasoning**: Strong performance on MMLU and GPQA  

---

## Knowledge Graph Integration
- All models and project selections are stored in **Neo4j AuraDB**.  
- Cypher queries link projects to chosen models.  
- Enables future research: track which models were chosen for similar problems and compare outcomes.

---

## Submission Notes
- `.kiro/` directory included at root.  
- Blog post published on AWS Builder Center (with screenshots).  
- Repo + blog link submitted via AI for Bharat dashboard.  

---

## Screenshots

## Screenshots

### Streamlit UI
![ui-screen](streamlit_ui.png)
![input](input.png)

### Recommendation Output
![output](output.png)

### Neo4j Graph
![cypher-output](cpher_output.png)
![neo4j-model-nodes](neo4j_output.png)
![added-node](newly_added_node.png)

---


## Acknowledgements
- Built as part of **AI for Bharat Week 6 Challenge: The Referee**.  
- Powered by **AWS Bedrock (Claude)**, **Neo4j AuraDB**, and **Streamlit**.  


---
